{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf79395-0b89-4d9f-af2c-19950ace1c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/spacy_gpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gcsfs\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.util import filter_spans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141d4fd1-546a-43b9-aa5d-2cc81bc0c164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'gs://doccano_annotation_2/data/doccano_export_10_10_2022.jsonl'\n",
    "drop_labels = ['BIOVERB']\n",
    "display = False\n",
    "write_all = True\n",
    "write_only_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0632d0c-968d-4e23-9920-99eb106c2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_jsonl(data_path):\n",
    "    gcs_file_system = gcsfs.GCSFileSystem()\n",
    "    with gcs_file_system.open(data_path, 'r', encoding=\"utf-8\") as f:\n",
    "        json_list = list(f)\n",
    "    data = []\n",
    "    for j in json_list:\n",
    "        data.append(json.loads(j))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def read_parquet(data_path):\n",
    "    return pd.read_parquet(data_path)\n",
    "\n",
    "def doccano_doc_to_df(df):\n",
    "    data = df[['doccano_doc']].copy()\n",
    "    data['entities'] = data['doccano_doc'].apply(lambda x: x['entities'])\n",
    "    data['text'] = data['doccano_doc'].apply(lambda x: x['text'])\n",
    "    data['relations'] = data['doccano_doc'].apply(lambda x: x['relations'])\n",
    "    return data[['text', 'entities', 'relations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a5ef94-cdb5-4021-baf5-784ec2ad4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path.split('.')[-1] == 'jsonl':\n",
    "    our_data = read_jsonl(data_path)\n",
    "elif data_path.split('.')[-1] == 'parquet':\n",
    "    our_data = read_parquet(data_path)\n",
    "    our_data = doccano_doc_to_df(our_data)\n",
    "else:\n",
    "    raise Error('Please provide jsonl or paquet format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d2a05a-b92e-445b-b0ef-2daece2d2612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Comments</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>Infection of the cell lines with M. arginini r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 17390, 'label': 'TARGET', 'start_offse...</td>\n",
       "      <td>[{'id': 9805, 'from_id': 34791, 'to_id': 17392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>Our results show that RA-NP inhibited LPS-indu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 17395, 'label': 'BIOVERB', 'start_offs...</td>\n",
       "      <td>[{'id': 2250, 'from_id': 17400, 'to_id': 17396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112</td>\n",
       "      <td>PF (10 μM) inhibited IL-33 production, Ca infl...</td>\n",
       "      <td>[{'id': 2, 'comment': 'Impossible a Tagger !!!...</td>\n",
       "      <td>[{'id': 17402, 'label': 'TARGET', 'start_offse...</td>\n",
       "      <td>[{'id': 2253, 'from_id': 17401, 'to_id': 17406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>We further showed that 4.1B inhibited the prol...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 17409, 'label': 'TARGET', 'start_offse...</td>\n",
       "      <td>[{'id': 2259, 'from_id': 17418, 'to_id': 17412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>Y(1) receptor antagonists, BIBP3226 and BIBO33...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'id': 17420, 'label': 'TARGET', 'start_offse...</td>\n",
       "      <td>[{'id': 9821, 'from_id': 17425, 'to_id': 17424...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  \\\n",
       "0  110  Infection of the cell lines with M. arginini r...   \n",
       "1  111  Our results show that RA-NP inhibited LPS-indu...   \n",
       "2  112  PF (10 μM) inhibited IL-33 production, Ca infl...   \n",
       "3  113  We further showed that 4.1B inhibited the prol...   \n",
       "4  114  Y(1) receptor antagonists, BIBP3226 and BIBO33...   \n",
       "\n",
       "                                            Comments  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [{'id': 2, 'comment': 'Impossible a Tagger !!!...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [{'id': 17390, 'label': 'TARGET', 'start_offse...   \n",
       "1  [{'id': 17395, 'label': 'BIOVERB', 'start_offs...   \n",
       "2  [{'id': 17402, 'label': 'TARGET', 'start_offse...   \n",
       "3  [{'id': 17409, 'label': 'TARGET', 'start_offse...   \n",
       "4  [{'id': 17420, 'label': 'TARGET', 'start_offse...   \n",
       "\n",
       "                                           relations  \n",
       "0  [{'id': 9805, 'from_id': 34791, 'to_id': 17392...  \n",
       "1  [{'id': 2250, 'from_id': 17400, 'to_id': 17396...  \n",
       "2  [{'id': 2253, 'from_id': 17401, 'to_id': 17406...  \n",
       "3  [{'id': 2259, 'from_id': 17418, 'to_id': 17412...  \n",
       "4  [{'id': 9821, 'from_id': 17425, 'to_id': 17424...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7963639-7480-48f1-8c75-7701e539a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_relations = our_data.relations.apply(lambda x: len(x))\n",
    "our_data = our_data.loc[len_relations > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc23df3-0b07-488d-a95c-afdf8d9199a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b734a6-2f71-45db-bf5b-04512f668e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities labels :  ['DISEASE', 'TISSUE', 'CELL_LINE', 'SMALL_MOLECULE', 'BIOVERB', 'CHEMICAL', 'PATHWAY', 'UNKNOWN', 'CELL LINE', 'TARGET']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/spacy_gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "ner_labels = our_data.entities.apply(lambda x: [x[i]['label'] for i in range(len(x))])\n",
    "print('Entities labels : ', list(set(np.sum([ner_labels.iloc[k] for k in range(ner_labels.shape[0])]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416e427f-22db-4242-a5fb-f5beb023b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations labels :  ['!induc', 'regul', 'suppress', 'interact', '!bind', 'inhibit', 'reduc', 'elevate', '!decreas', '!regul', 'induc', 'promote', 'imped', 'decreas', 'bind', '!increas', 'target', '!reduc', '!block', 'enhance', 'block', 'express', 'activ', 'increas', 'stimulate', '!activ', '!express', '!inhibit']\n"
     ]
    }
   ],
   "source": [
    "classes = our_data.relations.apply(lambda x: [x[i]['type'] for i in range(len(x))])\n",
    "print('Relations labels : ', list(set(np.sum([classes.iloc[k] for k in range(classes.shape[0])]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab6d399-eb53-4b16-b460-c655b6ee3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop labels that we are not interested in\n",
    "def drop_label(x, labels):\n",
    "    return [entity_dict for entity_dict in x if entity_dict['label'] not in labels]\n",
    "\n",
    "# Preprocess text input\n",
    "def preprocess_text(string):\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573f64a8-5e5d-49e9-9a60-9c4f3c1f1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "def get_tokens(x, i):\n",
    "    doc = x['doc'].copy()\n",
    "    text = str(doc[i])\n",
    "    start = doc[i:i+1].start_char\n",
    "    end = doc[i:i+1].end_char\n",
    "    if len(str(doc)) > end:\n",
    "        next_char = str(doc)[end]\n",
    "    else:\n",
    "        next_char = ''\n",
    "    if next_char == ' ':\n",
    "        ws = True\n",
    "    else:\n",
    "        ws = False\n",
    "    return {'text': text, \n",
    "            'start': start, \n",
    "            'end': end, \n",
    "            'id': i, \n",
    "            'ws': ws,\n",
    "            'disabled': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7aaa65-23d7-4fef-95c1-a66d4e0c1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spans\n",
    "def get_spans(x):\n",
    "    spans = []\n",
    "    doc = x['doc'].copy()\n",
    "    for d in x.entities:\n",
    "        start = d['start_offset']\n",
    "        end = d['end_offset']\n",
    "        substring = x['text'][start: end]\n",
    "        lspaces = len(substring) - len(substring.lstrip())\n",
    "        rspaces = len(substring) - len(substring.rstrip())\n",
    "        start += lspaces\n",
    "        end -= rspaces\n",
    "        span = doc.char_span(start, end, d['label'], kb_id=d['id'])\n",
    "        if span is not None:\n",
    "            spans.append(span)\n",
    "    filtered_spans = filter_spans(spans)\n",
    "    return [{'id': span.kb_id,\n",
    "            'text': str(span), \n",
    "            'start': span.start_char, \n",
    "            'token_start': span.start,\n",
    "            'token_end': span.end, \n",
    "            'end': span.end_char, \n",
    "            'type': 'span',\n",
    "            'label': span.label_}\n",
    "           for span in filtered_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a762118-fa7a-4e7d-98ee-77b7636f5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relations\n",
    "def get_head_child_attributes(spans, head_entity, child_entity, label):\n",
    "    head_dicts = [span_dict for span_dict in spans if (span_dict['id']==head_entity)]\n",
    "    child_dicts = [span_dict for span_dict in spans if (span_dict['id']==child_entity)]\n",
    "    \n",
    "    assert len(head_dicts) <= 1\n",
    "    assert len(child_dicts) <= 1\n",
    "\n",
    "    if (len(head_dicts) > 0) & (len(child_dicts) > 0):\n",
    "        head_start = head_dicts[0]['start']\n",
    "        head_end = head_dicts[0]['end']\n",
    "        head_token_start = head_dicts[0]['token_start']\n",
    "        head_token_end = head_dicts[0]['token_end']\n",
    "        head_label = head_dicts[0]['label']\n",
    "\n",
    "        child_start = child_dicts[0]['start']\n",
    "        child_end = child_dicts[0]['end']\n",
    "        child_token_start = child_dicts[0]['token_start']\n",
    "        child_token_end = child_dicts[0]['token_end']\n",
    "        child_label = child_dicts[0]['label']\n",
    "\n",
    "        head = head_token_end\n",
    "        child = child_token_end\n",
    "        label = label\n",
    "\n",
    "        relations_dict = {'head': head,\n",
    "                          'child': child,\n",
    "                          'head_span': {'start': head_start, \n",
    "                                       'end': head_end, \n",
    "                                       'token_start': head_token_start, \n",
    "                                       'token_end': head_token_end, \n",
    "                                       'label': head_label}, \n",
    "                            'child_span': {'start': child_start, \n",
    "                                       'end': child_end, \n",
    "                                       'token_start': child_token_start, \n",
    "                                       'token_end': child_token_end, \n",
    "                                       'label': child_label}, \n",
    "                            'label': label}\n",
    "    else:\n",
    "        relations_dict = {}\n",
    "    return relations_dict\n",
    "\n",
    "\n",
    "def get_relations(x, directed=True, add_no_rel=False, labels_to_drop=['target']):\n",
    "    relations_form = []\n",
    "    relations = x['relations'].copy()\n",
    "    spans = x['spans'].copy()\n",
    "    remaining_combinations = [(spans[i]['id'], spans[j]['id']) \n",
    "                        for i in range(len(spans))\n",
    "                        for j in range(len(spans))\n",
    "                        if spans[i]['id'] != spans[j]['id']]\n",
    "    for i in range(len(relations)):\n",
    "        head_entity = relations[i]['from_id'] \n",
    "        child_entity = relations[i]['to_id']\n",
    "        rel_label = x['relations'][i]['type'].split('!')[-1]\n",
    "        if rel_label not in labels_to_drop:\n",
    "            relations_dict = get_head_child_attributes(spans, head_entity, child_entity, rel_label)\n",
    "            if relations_dict != {}:\n",
    "                relations_form.append(relations_dict)\n",
    "                if not directed:\n",
    "                    relations_form.append(get_head_child_attributes(spans, child_entity, head_entity, \n",
    "                                                                    x['relations'][i]['type'].split('!')[-1]))\n",
    "            try:\n",
    "                remaining_combinations.remove((head_entity, child_entity))\n",
    "                if not directed:\n",
    "                    remaining_combinations.remove((child_entity, head_entity))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    if add_no_rel:\n",
    "        for c in range(len(remaining_combinations)):\n",
    "            head_entity = remaining_combinations[c][0]\n",
    "            child_entity = remaining_combinations[c][1]\n",
    "            relations_dict = get_head_child_attributes(spans, head_entity, child_entity, 'No-Rel')\n",
    "            relations_form.append(relations_dict)\n",
    "    return relations_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0efa809-db4c-4e55-9474-06a308f4457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_no_rel(x):\n",
    "    to_drop = False\n",
    "    labels = [x['relations_form'][i]['label'] for i in range(len(x['relations_form']))]\n",
    "    if (list(set(labels)) == ['No-Rel']) or (len(labels) == 0):\n",
    "        to_drop = True\n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2211d8-e87b-41d6-aebd-e2e703b8919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(our_data, drop_labels=[], directed=True, add_no_rel=False, rel_labels_to_drop=['target']):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "    if len(drop_labels) > 0:\n",
    "        our_data['entities'] = our_data['entities'].apply(lambda x: drop_label(x, drop_labels))\n",
    "    our_data['text_preprocessed'] = our_data['text'].str.lower()\n",
    "    our_data['doc'] = our_data['text_preprocessed'].apply(lambda x: nlp(x))\n",
    "    our_data['tokens'] = our_data.apply(lambda x: [get_tokens(x, i) for i in range(len(x.doc))], axis=1)\n",
    "    our_data['spans'] = our_data.apply(lambda x: get_spans(x), axis=1)\n",
    "    our_data['relations_form'] = our_data.apply(lambda x: get_relations(x, \n",
    "                                                                        directed=directed, \n",
    "                                                                        add_no_rel=add_no_rel, \n",
    "                                                                        labels_to_drop=rel_labels_to_drop), \n",
    "                                                axis=1)\n",
    "    our_data['to_drop'] = our_data.apply(lambda x: delete_rows_no_rel(x), axis=1)\n",
    "    our_data = our_data[our_data.to_drop==False].copy()\n",
    "    our_data['answer'] = 'accept'\n",
    "    our_data['meta'] = None\n",
    "    our_data['meta'] = our_data['meta'].apply(lambda x: {'source': 'unkown'})\n",
    "    formatted_data = our_data[['text_preprocessed', 'spans', 'tokens', 'relations_form', 'answer', 'meta']]\n",
    "    formatted_data.columns = ['text', 'spans', 'tokens', 'relations', 'answer', 'meta']\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1934b1ea-c712-4abf-874f-9a17564cc8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatted_data = preprocess(our_data, drop_labels=drop_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31214e14-6de4-4c88-a666-d427625e9d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9133600a-716f-4c94-b8c5-987023956761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display spans\n",
    "from spacy import displacy\n",
    "\n",
    "def display_entities(text, entities, entity_type=None):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for entity in entities:\n",
    "        if (entity[\"label\"] == entity_type) or (entity_type is None):\n",
    "            span_start = entity[\"start\"]\n",
    "            span_end = entity[\"end\"]\n",
    "            label = entity[\"label\"]\n",
    "            ent = doc.char_span(span_start, span_end, label=label)\n",
    "            if ent is None:\n",
    "                continue\n",
    "            ents.append(ent)\n",
    "    doc.ents = ents\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "if display:\n",
    "    not_working = []\n",
    "    for k in range(formatted_data.shape[0]):\n",
    "        row = formatted_data.iloc[k]\n",
    "        try:\n",
    "            display_entities(row.text, row.spans, None)\n",
    "        except ValueError:\n",
    "            not_working.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62a07125-7bbf-43b0-8991-ae2d202ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(df, path):\n",
    "    our_jsonl = df.to_dict(orient='records')\n",
    "    with open(path, 'w') as outfile:\n",
    "        for json_line in our_jsonl:\n",
    "            json.dump(json_line, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c720469-54ac-4d05-af30-e45b9bac769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_only_test:\n",
    "    assert write_all == False\n",
    "    write_jsonl(formatted_data, '../NER/assets/annotations_test.jsonl')\n",
    "    write_jsonl(formatted_data, '../RE/assets/annotations_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd2a5e82-1f1e-46b1-aa9f-2462a604c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_all:\n",
    "    np.random.seed(0)\n",
    "    n1 = int(formatted_data.shape[0] * 65/100)\n",
    "    n2 = int(formatted_data.shape[0] * 85/100) \n",
    "    df_shuffled = formatted_data.sample(frac=1)\n",
    "    annotations_train = df_shuffled[:n1]\n",
    "    annotations_dev = df_shuffled[n1:n2]\n",
    "    annotations_test = df_shuffled[n2:]\n",
    "    # Write for NER\n",
    "    write_jsonl(annotations_train, '../NER/assets/annotations_train.jsonl')\n",
    "    write_jsonl(annotations_dev, '../NER/assets/annotations_dev.jsonl')\n",
    "    write_jsonl(annotations_test, '../NER/assets/annotations_test.jsonl')\n",
    "    # Write for RE\n",
    "    write_jsonl(annotations_train, '../RE/assets/annotations_train.jsonl')\n",
    "    write_jsonl(annotations_dev, '../RE/assets/annotations_dev.jsonl')\n",
    "    write_jsonl(annotations_test, '../RE/assets/annotations_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d983a9a-9e48-46d8-be2b-aa5fdef1849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = formatted_data.relations.apply(lambda x: [x[i]['label'] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4257dcb-2c96-4501-969b-9be67a7be612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/spacy_gpu/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "total_labels = list(np.sum([labels.iloc[k] for k in range(labels.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfa26d5b-4638-433a-9816-b6a26f2bf981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activ 0.18608058608058609\n",
      "bind 0.07472527472527472\n",
      "block 0.04322344322344322\n",
      "decreas 0.0652014652014652\n",
      "elevate 0.0029304029304029304\n",
      "enhance 0.011721611721611722\n",
      "express 0.12747252747252746\n",
      "imped 0.007326007326007326\n",
      "increas 0.0695970695970696\n",
      "induc 0.08937728937728938\n",
      "inhibit 0.13553113553113552\n",
      "interact 0.0029304029304029304\n",
      "promote 0.008791208791208791\n",
      "reduc 0.07692307692307693\n",
      "regul 0.07179487179487179\n",
      "stimulate 0.014652014652014652\n",
      "suppress 0.011721611721611722\n"
     ]
    }
   ],
   "source": [
    "for label in np.unique(total_labels):\n",
    "    print(label, total_labels.count(label) / len(total_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255221d-011e-45a7-ac43-4d8b69cf00e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Pb : sur-représentation de la classe No-Rel\n",
    "\n",
    "Solutions possibles : \n",
    "\n",
    "    - Modifier la fonction de perte en pondérant le score en fonction de la classe\n",
    "    - Considérer des relations non dirigées : relations(head, child) = relation(child, head) ==> moins de No-Rel \n",
    "    - Concaténer deux modèles : cf https://doc.rero.ch/record/327157/files/cud_tci.pdf\n",
    "        - un premier modèle qui prédit l'absence de relation\n",
    "        - un deuxième qui prédit la classe de relation\n",
    "        \n",
    "        \n",
    "Remarque : \n",
    "- Pas de classe No-Rel !! Dans le modèle, on considère qu'il y a une relation A de e1 vers e2 ssi proba(A, e1, e2) > s=0.5. Si toutes les probas sont inférieures à s, alors pas de relation"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "spacy_gpu",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "spacy_gpu",
   "language": "python",
   "name": "spacy_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
